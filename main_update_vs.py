import time
from dotenv import load_dotenv
from typing import Union, Sequence, List


"""
Factory íŒ¨í„´
íŒŒì¼ í™•ì¥ì(pdf, txt, docx ë“±)ì— ë”°ë¼
 - ë‚´ë¶€ì ìœ¼ë¡œ ì ì ˆí•œ Loader ì„ íƒ
 ê²°ê³¼ëŠ” í•­ìƒ List[Document]
â†’ ìƒìœ„ ë¡œì§ì€ íŒŒì¼ ì¢…ë¥˜ë¥¼ ì‹ ê²½ ì“°ì§€ ì•ŠìŒ
"""
from loaddocuclass import CDocumentLoaderFactory

"""
ë‹¤ì–‘í•œ Chunk Splitter êµ¬í˜„ì²´ë“¤
ê³µí†µì :
 - ëª¨ë‘ CBaseChunkSplitter ì¸í„°í˜ì´ìŠ¤ë¥¼ ë”°ë¦„
í˜„ì¬ ì‹¤ì œë¡œ ì“°ëŠ” ê²ƒì€:
 - CSemanticTextSplitter
â†’ ì „ëµ íŒ¨í„´ (Strategy Pattern)
"""
from chunkingclass import ( 
    CBaseChunkSplitter, 
    CCharTextSplitter, 
    CRecursiveCharTextSplitter, 
    CTiktokenTextSplitter, 
    CTokenTextSplitter, 
    CSpacyTextSplitter, 
    CSentenceTransformersTokenTextSplitter,
    CNLTKTextSplitter,                          # Not completed
    CKONLPTextSplitter,                         # Not completed     
    CGPT2TokenizerFast,                         # Not completed
    CSemanticTextSplitter,
    CClauseTextSplitter,
    CHeaderTextSplitter,
)

"""
create_embedder
 - HuggingFace / OpenAI / ê¸°íƒ€ ëª¨ë¸ ìƒì„± íŒ©í† ë¦¬

EmbedderBase
 - embedderì˜ ê³µí†µ ì¸í„°í˜ì´ìŠ¤

LCEmbeddingAdapter
 - LangChain VectorStoreê°€ ìš”êµ¬í•˜ëŠ” Embeddings ì¸í„°í˜ì´ìŠ¤ë¡œ ë³€í™˜

ğŸ‘‰ â€œì„ë² ë”© ì—”ì§„â€ê³¼ â€œVectorStoreâ€ë¥¼ ëŠìŠ¨í•˜ê²Œ ê²°í•©
"""
from embeddingclass import LCEmbeddingAdapter
from embeddingclass import create_embedder
from embeddingclass import EmbedderBase

"""
VectorStoreConfig
 - backend / persist_dir / search ë°©ì‹ ë“± ì„¤ì • ê°ì²´

build_vectorstore
 - ì„¤ì • + embeddingì„ ë°›ì•„ ì‹¤ì œ VectorStore ìƒì„±

Document
 - LangChain í‘œì¤€ ë¬¸ì„œ íƒ€ì…
"""
from vectorstoreclass import VectorStoreConfig, build_vectorstore
from langchain_core.documents import Document

"""
íƒ€ì…ë³„ì¹­ì„¤ì • - ì…ë ¥ìœ ì—°ì„± í™•ë³´
- ì²­í‚¹ ë‹¨ê²Œì—ì„œ ë¬¸ìì—´ë¦¬ìŠ¤íŠ¸, Documentë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•¨
â†’ ì¬ì‚¬ìš©ì„± ì¦ê°€
"""
DocLike = Union[str, Document]
import sys
from chunking_splitter_reg import create_splitter



def build_indexing_context():
    load_dotenv()

    # 1) ì²­í‚¹ ë‹¨ê³„ ì¤€ë¹„: ChunkSplitter ìƒì„±
    """
    "char": CCharTextSplitter,
    "recursive_char": CRecursiveCharTextSplitter,
    "tiktoken": CTiktokenTextSplitter,
    "token": CTokenTextSplitter,
    "spacy": CSpacyTextSplitter,
    "sentence_transformers": CSentenceTransformersTokenTextSplitter,
    "semantic": CSemanticTextSplitter,
    "clause": CClauseTextSplitter,
    "header": CHeaderTextSplitter,
    """    
    splitter = create_splitter(
        "header",                 # splitter type
        #chunk_size=500,                   # chunk size
        #chunk_overlap=100,                # chunk overlap
    )

    # 2) ì„ë² ë”© ì—”ì§„ ì¤€ë¹„: ì„ë² ë” ìƒì„±
    embedder = create_embedder(
        "hf",                           # hf / openai / other
        model_name="BAAI/bge-m3",       # ëª¨ë¸ëª…
        device="mps",                   # cpu/gpu/mps
        normalize_embeddings=True,      # cosine similarity ìµœì í™”   
        use_e5_prefix=False,
    )
    # LangChain VectorStoreê°€ ìš”êµ¬í•˜ëŠ” Embeddings ì¸í„°í˜ì´ìŠ¤ë¡œ ë³€í™˜
    # ğŸ‘‰ Adapter Pattern
    lc_emb = LCEmbeddingAdapter(embedder)

    # 3) VectorStore ì¤€ë¹„: VectorStoreConfig ìƒì„±
    cfg = VectorStoreConfig(
        backend="faiss",                # faiss / chroma / pinecone
        persist_dir="./vs_faiss",       # ë¡œì»¬ ì €ì¥ ê²½ë¡œ: ë””ìŠ¤í¬ì— ì¸ë±ìŠ¤ ì €ì¥
        collection="my_docs",           # Chroma/Pineconeì—ì„œ ì£¼ë¡œ ì‚¬ìš©
        k=5,                            # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        search_type="similarity",       # similarity / exact
    )
    # ì„¤ì • + ì„ë² ë”©ì„ ê²°í•©í•´ VectorStore ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vs = build_vectorstore(cfg, lc_emb)

    return vs, splitter

def run_chunking(splitter: CBaseChunkSplitter, docs: Sequence[DocLike]):
    
    if docs and isinstance(docs[0], Document):
        texts = [doc.page_content for doc in docs]
    else:
        texts = docs

    texts = splitter.create_document(texts)

    return texts

def run_embedding(embedder: EmbedderBase, chunks: List[Document]) -> List[List[float]]:
    chunk_texts = [c.page_content for c in chunks]
    vectors = embedder.embed_documents(chunk_texts)
    return vectors

def index_one_file(vs, splitter, doc_path: str):
    # load â†’ chunk â†’ add_documents
    docs = CDocumentLoaderFactory().load(doc_path)
    texts = run_chunking(splitter, docs)
    ids = vs.add_documents(texts)
    return ids


if __name__ == "__main__":
    vs, splitter = build_indexing_context()
    
    files = [
        "./data/SPRI_Report.pdf",
        "./data/input.txt",
        "./data/finance.txt",
    ]

    for f in files:
        start = time.time()
        ids = index_one_file(vs, splitter, f)
        print(f"âœ… {f} ì¸ë±ì‹± ì™„ë£Œ: {len(ids)}ê°œ\t{time.time()-start:.2f}ì´ˆ")
    